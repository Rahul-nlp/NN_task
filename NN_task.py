# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1csfvNkj0ZPdPmO5IlcsltCMzzHZh98HW
"""

!pip install datasets
!pip install transformers
!pip install evaluate
!pip install scikit-learn

from typing import List, Tuple
from datasets import load_dataset, Dataset
import evaluate
import numpy as np
from sklearn.metrics import classification_report
from sklearn.utils.class_weight import compute_class_weight
import torch

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
    pipeline
)

MODEL_NAME = 'ai-forever/ruRoberta-large'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

DATASET_NAME = 'Davlan/sib200'
DATASET_LANGUAGE = 'rus_Cyrl'

train_set = load_dataset(DATASET_NAME, DATASET_LANGUAGE, split='train')
validation_set = load_dataset(DATASET_NAME, DATASET_LANGUAGE, split='validation')
test_set = load_dataset(DATASET_NAME, DATASET_LANGUAGE, split='test')

print(train_set)

MINIBATCH_SIZE = 32   # reduced for grad accumulation

tokenized_train_set = train_set.map(
    lambda it: tokenizer(it['text'], truncation=True),
    batched=True,
    batch_size=MINIBATCH_SIZE
)

tokenized_validation_set = validation_set.map(
    lambda it: tokenizer(it['text'], truncation=True),
    batched=True,
    batch_size=MINIBATCH_SIZE
)

cls_metric = evaluate.load('f1')

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return cls_metric.compute(
        predictions=predictions,
        references=labels,
        average='macro'
    )

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

list_of_categories = sorted(list(
    set(train_set['category']) |
    set(validation_set['category']) |
    set(test_set['category'])
))

indices_of_categories = list(range(len(list_of_categories)))
n_categories = len(list_of_categories)

id2label = dict(zip(indices_of_categories, list_of_categories))
label2id = dict(zip(list_of_categories, indices_of_categories))

print(f'Categories: {list_of_categories}')

labeled_train_set = tokenized_train_set.add_column(
    'label',
    [label2id[val] for val in tokenized_train_set['category']]
)

labeled_validation_set = tokenized_validation_set.add_column(
    'label',
    [label2id[val] for val in tokenized_validation_set['category']]
)

# ===== Class Weights (IMPORTANT) =====
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(labeled_train_set['label']),
    y=labeled_train_set['label']
)

class_weights = torch.tensor(class_weights, dtype=torch.float).cuda()

classifier = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=n_categories,
    id2label=id2label,
    label2id=label2id
).cuda()

# Freeze first 2 layers (stability)
for name, param in classifier.named_parameters():
    if name.startswith("roberta.encoder.layer.0") or \
       name.startswith("roberta.encoder.layer.1"):
        param.requires_grad = False

class WeightedTrainer(Trainer):
    def compute_loss(
        self,
        model,
        inputs,
        return_outputs=False,
        num_items_in_batch=None   # <-- FIX
    ):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits

        loss_fn = torch.nn.CrossEntropyLoss(
            weight=class_weights,
            label_smoothing=0.1
        )

        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

training_args = TrainingArguments(
    output_dir='rubert_sib200',

    learning_rate=1e-5,
    warmup_ratio=0.1,
    weight_decay=1e-3,

    per_device_train_batch_size=MINIBATCH_SIZE,
    per_device_eval_batch_size=MINIBATCH_SIZE,
    gradient_accumulation_steps=2,  # effective batch = 64

    num_train_epochs=16,
    fp16=True,

    eval_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='eval_f1',
    greater_is_better=True,

    report_to='none'
)

trainer = WeightedTrainer(
    model=classifier,
    args=training_args,
    train_dataset=labeled_train_set,
    eval_dataset=labeled_validation_set,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

classification_pipeline = pipeline(
    "text-classification",
    model=classifier,
    tokenizer=tokenizer,
    device=0
)

# ===== Validation =====
texts = list(validation_set["text"])

y_pred = [x["label"] for x in classification_pipeline(texts, batch_size=32)]
y_true = validation_set["category"]

print(classification_report(y_true=y_true, y_pred=y_pred))

# ===== Test =====
y_pred = [x["label"] for x in classification_pipeline(
    list(test_set["text"]), batch_size=32
)]
y_true = test_set["category"]

print(classification_report(y_true=y_true, y_pred=y_pred))